{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Test different MLP architectures. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidlaredorazo/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "# print(tf.__version__sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures\n",
    "\n",
    "Define each one of the different architectures to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(input_shape, output_shape):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,input_shape), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tf_model(X):\n",
    "    \n",
    "    l2_lambda_regularization = 0.20\n",
    "    l1_lambda_regularization = 0.10\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 20, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
    "                         kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"fc1\")\n",
    "    A2 = tf.layers.dense(A1, 20, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"fc2\")\n",
    "    y = tf.layers.dense(A2, 1, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"out\")\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_compiled_model(input_shape, output_shape):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X, y = create_placeholders(input_shape, output_shape)\n",
    "    \n",
    "    y_pred = tf_model(X)\n",
    "    cost = tf.losses.mean_squared_error(y, y_pred)\n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    total_cost = cost + reg_cost\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(total_cost)\n",
    "    \n",
    "    return {'X_placeholder':X, 'y_placeholder':y, 'y_pred':y_pred, 'cost':cost, 'total_cost':total_cost, 'optimizer':optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the usable models for this notebook\n",
    "\n",
    "#models = {'shallow-20-20':RULmodel_SN_4}\n",
    "\n",
    "#models = {'shallow-250-100':RULmodel_SN_4, 'shallow-100-50':RULmodel_SN_1, 'shallow-50-20':RULmodel_SN_2,\n",
    "#          'shallow-20-20':RULmodel_SN_3, 'shallow-20':RULmodel_SN_5, 'shallow-10':RULmodel_SN_6}\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 128\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, \n",
    "                                  window_size, window_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tModel = SequenceTunableModelRegression('ann-20-20', None, lib_type='keras', data_handler=dHandler_cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 128. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[-0.58075601 -0.0455243  -0.27982732 ... -0.81818182  0.43307087\n",
      "   0.4679733 ]\n",
      " [-0.35395189  0.0629156  -0.18014129 ... -0.45454545  0.25984252\n",
      "   0.25294702]\n",
      " [-0.21649485 -0.13299233 -0.13854003 ... -0.45454545  0.38582677\n",
      "   0.72049425]\n",
      " [-0.21649485 -0.39897698 -0.2299843  ... -0.45454545  0.08661417\n",
      "   0.29640676]\n",
      " [-0.20274914 -0.39590793 -0.05926217 ... -0.45454545  0.05511811\n",
      "   0.17880983]]\n",
      "[[128.]\n",
      " [128.]\n",
      " [128.]\n",
      " [128.]\n",
      " [128.]]\n",
      "Testing data (X, y)\n",
      "[[-0.65635739 -0.10946292 -0.48312402 ... -0.27272727  0.05511811\n",
      "   0.30947309]\n",
      " [ 0.03780069 -0.07365729 -0.27629513 ... -0.63636364  0.05511811\n",
      "   0.04416986]\n",
      " [ 0.13402062 -0.08644501  0.038854   ...  0.09090909  0.24409449\n",
      "   0.07882403]\n",
      " [-0.14776632  0.16828645  0.00431711 ...  0.09090909 -0.30708661\n",
      "   0.03365999]\n",
      " [-0.05841924  0.24654731  0.04317111 ... -0.09090909 -0.03937008\n",
      "   0.46996165]]\n",
      "[[112.]\n",
      " [ 98.]\n",
      " [ 69.]\n",
      " [ 82.]\n",
      " [ 91.]]\n"
     ]
    }
   ],
   "source": [
    "#For LSTM\n",
    "# tModel.data_handler.data_scaler = min_max_scaler\n",
    "# tModel.data_scaler = None\n",
    "\n",
    "#For ANN\n",
    "tModel.data_handler.data_scaler = None\n",
    "tModel.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.data_handler.sequence_length = 30\n",
    "#tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "tModel.data_handler.sequence_stride = 1\n",
    "tModel.data_handler.max_rul = 128\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model ann-20-20\n",
      "Computing for dataset 1\n",
      "Epoch:Final cost_reg= 211.684462653 cost= 182.337556627\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 203.018555959 cost= 170.390945435\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 208.612341563 cost= 176.371859656\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 208.403184679 cost= 175.431611379\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 204.973952823 cost= 172.253301409\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 213.966761695 cost= 183.557486640\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 210.278204176 cost= 181.552694109\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 207.504407247 cost= 175.817009396\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 207.287563324 cost= 177.869756911\n",
      "tensorflow test\n",
      "Epoch:Final cost_reg= 212.729979197 cost= 179.608186510\n",
      "tensorflow test\n",
      "Results for model ann-20-20\n",
      "DescribeResult(nobs=10, minmax=(array([14.59897257]), array([15.39285549])), mean=array([14.98109115]), variance=array([0.05767564]), skewness=array([0.16166676]), kurtosis=array([-0.65879159]))\n",
      "DescribeResult(nobs=10, minmax=(array([3.3581355]), array([4.20972117])), mean=array([3.74814764]), variance=array([0.09859676]), skewness=array([0.28745507]), kurtosis=array([-1.4640651]))\n",
      "DescribeResult(nobs=10, minmax=(array([29.527748]), array([30.420789])), mean=array([30.083538]), variance=array([0.09772185]), skewness=array([-0.60074035]), kurtosis=array([-0.94414002]))\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "tModel.epochs = 200\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "num_features = len(selected_features)\n",
    "\n",
    "windowSize = 24\n",
    "windowStride = 1\n",
    "constRul = 128\n",
    "\n",
    "file = open(\"results/MLP/ResultsDataset_1_tf.csv\", \"w\")\n",
    "csvfile = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "models = {'ann-20-20':tf_compiled_model}\n",
    "\n",
    "for key, model_def in models.items():\n",
    "  \n",
    "    print(\"For model \"+str(key))\n",
    "    #file.write(\"For model \"+str(key)+'\\n\\n')\n",
    "  \n",
    "    for i in range(1,2):\n",
    "\n",
    "        dataset = i\n",
    "        print(\"Computing for dataset \"+str(i))\n",
    "        #file.write(\"Computing for dataset \"+str(i)+'\\n\\n')\n",
    "      \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "      \n",
    "        input_shape = windowSize*num_features #For simple ANN\n",
    "      \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = windowSize\n",
    "        tModel.data_handler.sequence_stride = windowStride\n",
    "        tModel.data_handler.max_rul = constRul\n",
    "        tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "        #tModel.print_data()\n",
    "\n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            model = model_def(input_shape, 1)\n",
    "            tModel.change_model('ModelRUL_SN_1', model, 'tensorflow')\n",
    "            #model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "\n",
    "            sess = tf.Session()\n",
    "            tModel.train_model(verbose=0, tf_session=sess)\n",
    "            #tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2, tf_session=sess)\n",
    "            sess.close()\n",
    "            #print(\"scores\")\n",
    "          \n",
    "            #print(j)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            #rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "          \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "\n",
    "        print(\"Results for model \" + key)\n",
    "  \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "          \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating statistics for model shallow-20-20\n",
      "Working on dataset 1\n",
      "238\n",
      "100/100 [==============================] - 1s 9ms/step\n",
      "100/100 [==============================] - 1s 9ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "Results for model shallow-20-20\n",
      "DescribeResult(nobs=10, minmax=(array([16.74604431]), array([17.23861943])), mean=array([17.06463444]), variance=array([0.0325017]), skewness=array([-0.88353254]), kurtosis=array([-0.61935145]))\n",
      "DescribeResult(nobs=10, minmax=(array([8.08271055]), array([9.26545247])), mean=array([8.64814575]), variance=array([0.20635946]), skewness=array([-0.04830206]), kurtosis=array([-1.50147762]))\n",
      "DescribeResult(nobs=10, minmax=(array([17.28190351]), array([20.75944268])), mean=array([18.59064881]), variance=array([1.26079365]), skewness=array([0.53965178]), kurtosis=array([-0.70071753]))\n",
      "Working on dataset 2\n",
      "238\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "Results for model shallow-20-20\n",
      "DescribeResult(nobs=10, minmax=(array([29.77055112]), array([30.12278605])), mean=array([29.9410427]), variance=array([0.01371166]), skewness=array([0.00427771]), kurtosis=array([-1.15611101]))\n",
      "DescribeResult(nobs=10, minmax=(array([54.9013126]), array([63.53765616])), mean=array([58.82387766]), variance=array([5.75062991]), skewness=array([0.46125081]), kurtosis=array([-0.08556425]))\n",
      "DescribeResult(nobs=10, minmax=(array([39.96110995]), array([45.92934572])), mean=array([43.77577243]), variance=array([4.56499441]), skewness=array([-1.02919672]), kurtosis=array([-0.35078346]))\n",
      "Working on dataset 3\n",
      "238\n",
      "100/100 [==============================] - 1s 11ms/step\n",
      "100/100 [==============================] - 1s 12ms/step\n",
      "100/100 [==============================] - 1s 12ms/step\n",
      "100/100 [==============================] - 1s 12ms/step\n",
      "100/100 [==============================] - 1s 12ms/step\n",
      "100/100 [==============================] - 1s 12ms/step\n",
      "100/100 [==============================] - 1s 13ms/step\n",
      "100/100 [==============================] - 1s 13ms/step\n",
      "100/100 [==============================] - 1s 13ms/step\n",
      "100/100 [==============================] - 1s 13ms/step\n",
      "Results for model shallow-20-20\n",
      "DescribeResult(nobs=10, minmax=(array([16.9440255]), array([18.5291662])), mean=array([17.77146405]), variance=array([0.1556282]), skewness=array([-0.24678058]), kurtosis=array([1.09323979]))\n",
      "DescribeResult(nobs=10, minmax=(array([6.02927943]), array([10.18819117])), mean=array([7.64747814]), variance=array([1.88430213]), skewness=array([0.67730309]), kurtosis=array([-0.75931237]))\n",
      "DescribeResult(nobs=10, minmax=(array([25.60197198]), array([29.56520624])), mean=array([28.14127926]), variance=array([2.03920931]), skewness=array([-0.82161631]), kurtosis=array([-1.00131736]))\n",
      "Working on dataset 4\n",
      "238\n",
      "248/248 [==============================] - 1s 5ms/step\n",
      "248/248 [==============================] - 1s 5ms/step\n",
      "248/248 [==============================] - 1s 5ms/step\n",
      "248/248 [==============================] - 1s 5ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "248/248 [==============================] - 1s 6ms/step\n",
      "Results for model shallow-20-20\n",
      "DescribeResult(nobs=10, minmax=(array([33.95668493]), array([35.46329315])), mean=array([35.14841976]), variance=array([0.20013415]), skewness=array([-2.14765661]), kurtosis=array([3.28938939]))\n",
      "DescribeResult(nobs=10, minmax=(array([58.71171799]), array([67.91744227])), mean=array([65.78258587]), variance=array([6.98363345]), skewness=array([-2.12042295]), kurtosis=array([3.41637688]))\n",
      "DescribeResult(nobs=10, minmax=(array([58.69075155]), array([70.03360109])), mean=array([64.19236988]), variance=array([12.78385268]), skewness=array([0.16367492]), kurtosis=array([-1.08353196]))\n"
     ]
    }
   ],
   "source": [
    "datasets = [1,2,3,4]\n",
    "iterations = 10\n",
    "tModel.epochs = 150\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "window_sizes = {1:17,2:17,3:17,4:17}\n",
    "strides = {1:1,2:1,3:1,4:1}\n",
    "max_ruls = {1:129, 2:129, 3:129, 4:129}\n",
    "num_features = len(selected_features)\n",
    "\n",
    "input_shape = None\n",
    "models = {'shallow-20-20':RULmodel_SN_4}\n",
    "\n",
    "#For each model\n",
    "for key, model_def in models.items():\n",
    "    file = open(\"results/MLP/ResultsDatasets_singleSet\"+key+\".csv\", \"w\")\n",
    "    csvfile = csv.writer(file, lineterminator='\\n')\n",
    "    \n",
    "    print(\"Generating statistics for model \" + key)\n",
    "\n",
    "    #For each dataset\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        print(\"Working on dataset \" + str(i))\n",
    "        \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "        \n",
    "        input_shape = window_sizes[i]*num_features #For simple ANN\n",
    "        #input_shape = (window_sizes[i],num_features) #For RNN\n",
    "        \n",
    "        print(input_shape)\n",
    "        \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = window_sizes[i]\n",
    "        tModel.data_handler.sequence_stride = strides[i]\n",
    "        tModel.data_handler.max_rul = max_ruls[i]\n",
    "        tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "        #tModel.print_data()\n",
    "        \n",
    "        #tModel.print_data()\n",
    "        \n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "\n",
    "            tModel.change_model(key, model, 'keras')\n",
    "            tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "            #print(\"scores\")\n",
    "            \n",
    "            #print(j)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "            \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "            \n",
    "        print(\"Results for model \" + key)\n",
    "    \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "            \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
