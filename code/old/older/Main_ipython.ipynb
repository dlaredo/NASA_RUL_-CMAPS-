{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 14)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:141: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\", name=\"C1\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:142: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\", name=\"C2\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:143: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\", name=\"C3\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:144: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\", name=\"C4\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:148: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 1), activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\", name=\"Clast\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:155: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, name=\"fc\", activation=\"tanh\", kernel_initializer=\"glorot_normal\")`\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:164: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17731 samples, validate on 100 samples\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 1s 32us/step - loss: 6554.7945 - val_loss: 4958.7324\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5620.1881 - val_loss: 4534.6870\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5173.8787 - val_loss: 4162.1118\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4773.8147 - val_loss: 3824.4128\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4408.0566 - val_loss: 3512.8965\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4071.1164 - val_loss: 3226.0068\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3760.8594 - val_loss: 2963.7866\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 3475.1612 - val_loss: 2725.5068\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3210.1870 - val_loss: 2502.6785\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2965.1935 - val_loss: 2294.1494\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2736.4515 - val_loss: 2091.8904\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2524.9543 - val_loss: 1902.9069\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2329.8856 - val_loss: 1769.2985\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2150.7907 - val_loss: 1592.9528\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1981.8628 - val_loss: 1451.2590\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1831.6872 - val_loss: 1320.7247\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1691.3930 - val_loss: 1205.2455\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1560.2075 - val_loss: 1101.6086\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1443.7695 - val_loss: 1017.7780\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1352.3808 - val_loss: 993.4927\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1244.9644 - val_loss: 863.9763\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1156.4667 - val_loss: 780.2006\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1062.1150 - val_loss: 721.2880\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 985.3243 - val_loss: 656.3938\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 918.3178 - val_loss: 630.4851\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 854.9731 - val_loss: 542.9254\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 790.1013 - val_loss: 504.3832\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 739.8080 - val_loss: 468.3199\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 694.3318 - val_loss: 431.3748\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 644.1464 - val_loss: 457.4545\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 611.7805 - val_loss: 372.2050\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 571.0677 - val_loss: 350.6248\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 537.3723 - val_loss: 324.9049\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 505.1472 - val_loss: 314.1995\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 469.1048 - val_loss: 290.6650\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 464.2561 - val_loss: 289.3251\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 444.5059 - val_loss: 271.1227\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 416.4082 - val_loss: 247.2072\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 389.9730 - val_loss: 233.9033\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 713.0572 - val_loss: 600.4155\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 547.7123 - val_loss: 418.5570\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 469.8629 - val_loss: 370.5980\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 441.0224 - val_loss: 333.8884\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 421.8664 - val_loss: 298.9846\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 391.5933 - val_loss: 268.3009\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 367.1604 - val_loss: 234.0156\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 346.5382 - val_loss: 211.2002\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 319.2776 - val_loss: 203.8137\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 313.2014 - val_loss: 194.3004\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 300.2970 - val_loss: 183.0462\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 304.6927 - val_loss: 304.1993\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 285.5285 - val_loss: 182.4887\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 281.6017 - val_loss: 180.0389\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 265.8401 - val_loss: 176.0403\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 293.3614 - val_loss: 175.9376\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 260.2500 - val_loss: 173.2074\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 256.7056 - val_loss: 189.3461\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 256.7206 - val_loss: 168.5922\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 240.5516 - val_loss: 192.1795\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 234.7200 - val_loss: 164.8654\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 256.2313 - val_loss: 170.2023\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 244.5775 - val_loss: 166.9406\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 222.9136 - val_loss: 163.1437\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 224.0468 - val_loss: 161.4653\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 236.5994 - val_loss: 166.1670\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 222.1849 - val_loss: 167.6972\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 232.6816 - val_loss: 194.6245\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 211.1034 - val_loss: 171.0414\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 235.5114 - val_loss: 197.4063\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 205.4491 - val_loss: 148.6228\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 219.5707 - val_loss: 172.1168\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 211.4941 - val_loss: 218.5986\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 216.8952 - val_loss: 180.7243\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 209.3307 - val_loss: 179.4967\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 216.9465 - val_loss: 228.7159\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 195.4520 - val_loss: 156.1523\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 211.4894 - val_loss: 183.9736\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 192.5434 - val_loss: 198.6824\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 219.0437 - val_loss: 178.3968\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 196.9481 - val_loss: 188.5345\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 198.5363 - val_loss: 223.9199\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 200.3998 - val_loss: 201.8222\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 198.9986 - val_loss: 162.4883\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.0356 - val_loss: 197.8236\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 206.3450 - val_loss: 154.0880\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.8321 - val_loss: 179.7445\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 199.2340 - val_loss: 287.0073\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 185.7879 - val_loss: 202.0335\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 193.8919 - val_loss: 212.3815\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 189.1745 - val_loss: 178.8154\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.2112 - val_loss: 156.5882\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 201.9523 - val_loss: 210.7947\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 178.9904 - val_loss: 170.4940\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 194.0650 - val_loss: 152.0757\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.3225 - val_loss: 223.9234\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.4901 - val_loss: 171.8260\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.9919 - val_loss: 237.9090\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 183.7402 - val_loss: 182.3343\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 180.8878 - val_loss: 174.9927\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.5227 - val_loss: 160.7605\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 180.6352 - val_loss: 207.9736\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.9145 - val_loss: 169.4570\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 173.8504 - val_loss: 155.8632\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.8912 - val_loss: 165.4314\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 184.5107 - val_loss: 173.7495\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 171.2106 - val_loss: 149.4984\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 178.2897 - val_loss: 182.1929\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.5537 - val_loss: 232.1266\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 190.7142 - val_loss: 164.7363\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 173.2458 - val_loss: 192.2769\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 173.2100 - val_loss: 175.4830\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.8598 - val_loss: 200.1113\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.5561 - val_loss: 205.2684\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 177.3306 - val_loss: 158.8228\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.0409 - val_loss: 160.1707\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.2320 - val_loss: 159.6004\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 192.2500 - val_loss: 174.5473\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.9382 - val_loss: 183.9250\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 163.5144 - val_loss: 211.3366\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.9690 - val_loss: 149.6163\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 165.4917 - val_loss: 180.2632\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 160.3465 - val_loss: 253.9214\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 192.9738 - val_loss: 155.7886\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.5743 - val_loss: 163.9086\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 165.7329 - val_loss: 155.6319\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 167.3694 - val_loss: 232.0640\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.9648 - val_loss: 162.8291\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 161.0094 - val_loss: 161.4412\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 183.1465 - val_loss: 152.7848\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.6361 - val_loss: 209.1324\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.9482 - val_loss: 160.8953\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.0071 - val_loss: 194.9923\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 165.4339 - val_loss: 153.6665\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.4424 - val_loss: 162.0406\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 171.9036 - val_loss: 175.9369\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.6063 - val_loss: 159.6476\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.3008 - val_loss: 186.4275\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 154.2109 - val_loss: 221.3018\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 163.2049 - val_loss: 150.2671\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.8539 - val_loss: 196.8441\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.2494 - val_loss: 158.1765\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 171.7039 - val_loss: 164.9508\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 153.7633 - val_loss: 273.9819\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 162.3437 - val_loss: 153.7024\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.3251 - val_loss: 152.0893\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 156.9629 - val_loss: 157.3750\n",
      "Epoch 147/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.9374 - val_loss: 195.8936\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.7742 - val_loss: 184.8046\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 165.8624 - val_loss: 180.5034\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.8582 - val_loss: 157.4161\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.3057 - val_loss: 160.8432\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 156.7853 - val_loss: 163.6556\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.3672 - val_loss: 157.5548\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.7531 - val_loss: 169.2614\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.1786 - val_loss: 164.5968\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.5981 - val_loss: 155.3090\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 188.3727 - val_loss: 174.1939\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.5243 - val_loss: 151.1416\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.5235 - val_loss: 197.7053\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.1580 - val_loss: 150.7776\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.0316 - val_loss: 169.8493\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.9516 - val_loss: 167.3178\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.6005 - val_loss: 171.5589\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 156.3263 - val_loss: 155.9079\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.0691 - val_loss: 165.7454\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.8339 - val_loss: 155.7443\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.4136 - val_loss: 178.5819\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.8647 - val_loss: 180.0891\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.9839 - val_loss: 155.2796\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.0320 - val_loss: 163.4529\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.0976 - val_loss: 161.2415\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.2350 - val_loss: 170.8719\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.2732 - val_loss: 165.3343\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.7189 - val_loss: 156.6164\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 165.7375 - val_loss: 155.6497\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.6727 - val_loss: 171.7455\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.3123 - val_loss: 153.6294\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.7860 - val_loss: 164.7159\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.4767 - val_loss: 180.3595\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.5862 - val_loss: 165.3212\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.4883 - val_loss: 163.3736\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 153.6355 - val_loss: 160.0415\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.4201 - val_loss: 186.8625\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.2883 - val_loss: 195.3893\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.0619 - val_loss: 256.3518\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.8318 - val_loss: 178.5684\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 152.9732 - val_loss: 180.3257\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.8763 - val_loss: 173.4399\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.2371 - val_loss: 161.1539\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.3775 - val_loss: 186.5513\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 159.3874 - val_loss: 158.7212\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.9241 - val_loss: 191.5347\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.9663 - val_loss: 162.1149\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.1661 - val_loss: 158.9609\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.3009 - val_loss: 161.2610\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.4128 - val_loss: 183.2910\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.3273 - val_loss: 153.2734\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.4659 - val_loss: 198.0723\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.2384 - val_loss: 188.6713\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 146.3392 - val_loss: 217.3584\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.4090 - val_loss: 172.0000\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.7160 - val_loss: 168.6970\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.1461 - val_loss: 172.5200\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.4893 - val_loss: 177.5138\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.1430 - val_loss: 174.7348\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1733 - val_loss: 177.8282\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.2996 - val_loss: 169.3034\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.9857 - val_loss: 168.1576\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.8543 - val_loss: 167.4442\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1339 - val_loss: 169.6592\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.2830 - val_loss: 175.2981\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.5259 - val_loss: 177.6047\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.8948 - val_loss: 175.0977\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1462 - val_loss: 174.8624\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.7237 - val_loss: 174.6445\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.1266 - val_loss: 170.0457\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 134.3201 - val_loss: 179.4398\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.1745 - val_loss: 183.5758\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.7882 - val_loss: 171.6418\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.2712 - val_loss: 169.6172\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.7458 - val_loss: 167.6307\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.0879 - val_loss: 169.8834\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.6803 - val_loss: 176.2135\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.8354 - val_loss: 170.5703\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.3752 - val_loss: 174.2134\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.4966 - val_loss: 170.9823\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.1325 - val_loss: 182.3448\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.2701 - val_loss: 174.4261\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.3059 - val_loss: 181.9034\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.4969 - val_loss: 171.4009\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.6604 - val_loss: 177.9931\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.8360 - val_loss: 170.0780\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.5746 - val_loss: 170.9546\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.7146 - val_loss: 180.6510\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.2095 - val_loss: 170.6451\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.5597 - val_loss: 168.4054\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.6726 - val_loss: 171.8498\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.1514 - val_loss: 181.5091\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.5613 - val_loss: 173.9692\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.6819 - val_loss: 174.1828\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.7723 - val_loss: 182.3777\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.0130 - val_loss: 177.7372\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.6873 - val_loss: 180.9793\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.1329 - val_loss: 172.7512\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.3822 - val_loss: 168.4332\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.9716 - val_loss: 175.1516\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.7162 - val_loss: 169.6778\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.4115 - val_loss: 168.9769\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.4181 - val_loss: 172.7556\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.3010 - val_loss: 182.1075\n",
      "100/100 [==============================] - 0s 30us/step\n",
      "Test score: 182.10751342773438\n",
      "[[ 13.49472169   0.        ]\n",
      " [337.18161011   0.        ]\n",
      " [ 97.6286188    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, math, random, pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler,EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Input, merge, Convolution2D, Reshape, Flatten, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    s=0\n",
    "    for i in range(len(y_true)):\n",
    "        d = y_pred[i] - y_true[i]\n",
    "        if d < 0:\n",
    "            s+=math.e**(-d/13)-1\n",
    "        else:\n",
    "            s+=math.e**(d/10)-1\n",
    "    return s\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lrat = 0\n",
    "    if epoch<200:\n",
    "        lrat = 0.001\n",
    "    else:\n",
    "        lrat = 0.0001\n",
    "    return lrat  \n",
    "\n",
    "FeatureN = 14\n",
    "nb_epoch = 250\n",
    "batch_size = 512\n",
    "FilterN = 10\n",
    "FilterL = 10\n",
    "rmse,sco,tm = [], [], []\n",
    "\n",
    "\n",
    "#writer = open('DCNN_5C_30TW_noRearly.pkl', 'wb')\n",
    "ConstRUL = 125\n",
    "TW = 30\n",
    "Dataset = '1'\n",
    "# Time Window Max of Dataset\n",
    "#1:31, 2:21, 3:38, 4:19\n",
    "#1:125, 2: 155, 3:125ï¼Œ 4: 155\n",
    "\n",
    "############ training samples ##################################\n",
    "\n",
    "setTrain = {'1':100, '2':260, '3':100, '4':248}\n",
    "setTest = {'1':100, '2':259, '3':100, '4':248}\n",
    "nTrain = setTrain[Dataset]\n",
    "nTest = setTest[Dataset]\n",
    "\n",
    "data = [] \n",
    "for line in open(\"../CMAPSSData/train_FD00\"+Dataset+\".txt\"):\n",
    "    data.append(line.split())\n",
    "data=np.array(data)\n",
    "data = np.cast['float64'](data)\n",
    "data_copy = data\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "data = min_max_scaler.fit_transform(data)\n",
    "num=[]\n",
    "for i in range(nTrain):\n",
    "    tmp = data[np.where(data_copy[:,0]==i+1),:][0][:, np.array([6,7,8,11,12,13,15,16,17,18,19,21,24,25])]\n",
    "    num.append(tmp)\n",
    "num=np.array(num)\n",
    "\n",
    "label=[]\n",
    "for i in range(nTrain):\n",
    "    label.append([])\n",
    "    length = len(num[i])\n",
    "    for j in range(length):\n",
    "        label[i].append(ConstRUL if length-j-1>=ConstRUL else length-j-1)\n",
    "label = np.array(label)\n",
    "\n",
    "samples,targets,noofsample = [],[],[]\n",
    "for i in range(nTrain):\n",
    "    noofsample.append(len(num[i])-TW+1)\n",
    "    for j in range(noofsample[-1]):\n",
    "        samples.append(num[i][j:j+TW,:])\n",
    "        targets.append(label[i][j+TW-1])\n",
    "samples = np.array(samples)\n",
    "targets = np.array(targets)\n",
    "\n",
    "################## testing data ###########################\n",
    "data = [] \n",
    "for line in open(\"../CMAPSSData/test_FD00\"+Dataset+\".txt\"):\n",
    "    data.append(line.split())\n",
    "data=np.array(data)\n",
    "data = np.cast['float64'](data)\n",
    "data_copy = data\n",
    "data = min_max_scaler.transform(data)\n",
    "numt=[]\n",
    "for i in range(nTest):\n",
    "    tmp = data[np.where(data_copy[:,0]==i+1),:][0][:, np.array([6,7,8,11,12,13,15,16,17,18,19,21,24,25])]\n",
    "    numt.append(tmp)\n",
    "numt=np.array(numt)\n",
    "\n",
    "samplet, count_miss = [],[]\n",
    "for i in range(nTest):\n",
    "    if len(numt[i])>=TW:\n",
    "        samplet.append(numt[i][-TW:,:])\n",
    "    else:\n",
    "        count_miss.append(i)\n",
    "samplet = np.array(samplet)\n",
    "\n",
    "labelt = [] \n",
    "for line in open(\"../CMAPSSData/RUL_FD00\"+Dataset+\".txt\"):\n",
    "    labelt.append(line.split())\n",
    "labelt = np.cast['int32'](labelt)\n",
    "labelnew = []\n",
    "for i in range(nTest):\n",
    "    if i not in count_miss:\n",
    "        #labelnew.append(labelt[i][0])\n",
    "        labelnew.append(labelt[i][0] if labelt[i][0]<=ConstRUL else ConstRUL)\n",
    "labelt = labelnew\n",
    "labelt=np.array(labelt)\n",
    "\n",
    "seed = 2222\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(samples)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(targets)\n",
    "samplet = samplet[np.argsort(labelt)]\n",
    "labelt = labelt[np.argsort(labelt)]\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    start = time.clock()\n",
    "    input_layer = Input(shape=(TW, FeatureN))\n",
    "    y = Reshape((TW, FeatureN, 1), input_shape=(TW, FeatureN, ),name = 'Reshape')(input_layer)\n",
    "\n",
    "    y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C1')(y)\n",
    "    y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C2')(y)\n",
    "    y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C3')(y)\n",
    "    y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C4')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C5')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C6')(y)\n",
    "    \n",
    "    y = Convolution2D(1, 3, 1, border_mode='same', init='glorot_normal', activation='tanh', name='Clast')(y)  \n",
    "    \n",
    "    y = keras.layers.Reshape((TW,14))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(100, activation='tanh', init='glorot_normal', activity_regularizer=keras.regularizers.l2(0.01),)(y)\n",
    "    y = Dense(100,activation='tanh', init='glorot_normal', name='fc')(y)\n",
    "    y = Dense(1)(y)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0, beta_1=0.5)\n",
    "    DCNN = Model([input_layer], [y])\n",
    "    #DCNN.compile(loss=get_score,optimizer=opt)\n",
    "    DCNN.compile(loss='mean_squared_error',optimizer=opt)\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "                     validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "    \n",
    "    #, TensorBoard(log_dir='tmp\\\\tan_4c_4')\n",
    "    \n",
    "    #history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "    #                 validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "    #history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=0, callbacks=[lrate])\n",
    "    \n",
    "    score = DCNN.evaluate(samplet, labelt, batch_size=batch_size, verbose=1)\n",
    "    print('Test score:', score)\n",
    "    end = time.clock()\n",
    "    \n",
    "    rmse.append(np.sqrt(score))\n",
    "    sco.append(get_score(labelt, DCNN.predict(samplet)))\n",
    "    tm.append(end-start)\n",
    "    \n",
    "    #DCNN.save('DCNN_5C_noR_'+str(i)+'.h5')\n",
    "    #DCNN.save_weights('DCNN_5C_'+str(i))\n",
    "\n",
    "rec = np.array([[np.average(rmse), np.std(rmse)],[np.average(sco), np.std(sco)],[np.average(tm), np.std(tm)]])\n",
    "print(rec)\n",
    "#pickle.dump(rec, writer)\n",
    "\n",
    "#np.savetxt('file.txt', rec, delimiter=' ')\n",
    "#DCNN.save_weights('DCNN_5C')\n",
    "\n",
    "#writer.close()\n",
    "\n",
    "#writer = open('Dataset1_30TW_data.pkl', 'wb')\n",
    "#pickle.dump([samplet, labelt], writer)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(cycles, rulArray, 'bo-', label='RUL')\n",
    "    plt.plot(cycles, nnPred, 'go-', label='NN Pred')\n",
    "    plt.plot(cycles, cnnPred, 'ro-', label='CNN Pred')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (Cycle)\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.title(\"Test Engine Unit #{}\".format(engineUnit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "for line in open(\"../CMAPSSData/test_FD00\"+Dataset+\".txt\"):\n",
    "    data.append(line.split())\n",
    "data=np.array(data)\n",
    "data = np.cast['float64'](data)\n",
    "data_copy = data\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "data = min_max_scaler.fit_transform(data)\n",
    "num=[]\n",
    "for i in range(nTrain):\n",
    "    tmp = data[np.where(data_copy[:,0]==i+1),:][0][:, np.array([6,7,8,11,12,13,15,16,17,18,19,21,24,25])]\n",
    "    num.append(tmp)\n",
    "num=np.array(num)\n",
    "\n",
    "samples,targets,noofsample = [],[],[]\n",
    "for i in range(nTrain):\n",
    "    noofsample.append(len(num[i])-TW+1)\n",
    "    for j in range(noofsample[-1]):\n",
    "        samples.append(num[i][j:j+TW,:])\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10196, 30, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"Testing data\")\\nprint(X_test2.shape)\\nprint(X_test2[-5:,:])\\nprint(nnPred)\\nprint(cnnPred)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engineUnit = 21\n",
    "\n",
    "print(samples.shape)\n",
    "\n",
    "\n",
    "'''cnnPred = DCNN.predict(samplet)\n",
    "\n",
    "maxCycle = X_test2.shape[0]\n",
    "faultCycle = y_test[engineUnit-1]\n",
    "cycles = np.arange(maxCycle)\n",
    "rulArray = np.arange(faultCycle, maxCycle+faultCycle)\n",
    "rulArray[rulArray > constRUL] = constRUL\n",
    "rulArray = np.flipud(rulArray)'''\n",
    "\n",
    "#print(cycles)\n",
    "#print(rulArray)\n",
    "\n",
    "'''print(\"Testing data\")\n",
    "print(X_test2.shape)\n",
    "print(X_test2[-5:,:])\n",
    "print(nnPred)\n",
    "print(cnnPred)'''\n",
    "\n",
    "#plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
